{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be7fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from itertools import chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74141ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, conl_text):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Create a dictionary to store the data\n",
    "        self.data={'id':[], 'labels':[], 'sentences':[]}\n",
    "        data=[]\n",
    "        with open(conl_text, 'r') as f:\n",
    "            data=f.read()\n",
    "            data=data.split('\\n\\n')\n",
    "            \n",
    "        self.lines=[]\n",
    "        for lino in range(len(data)):\n",
    "            lino=data[lino].split('\\n')[1:]\n",
    "            self.lines.append(lino)\n",
    "            \n",
    "                            \n",
    "        for line in self.lines:\n",
    "            texts=[]\n",
    "            labels=[]\n",
    "            for index, word in enumerate(line):\n",
    "                if index==0:\n",
    "                    part=word.split()\n",
    "                    self.data['id'].append(part[2])\n",
    "                \n",
    "                else:\n",
    "                    split_line=word.split('_')\n",
    "                    split_line=[x.strip() for x in split_line]\n",
    "                    split_line=list(filter(lambda x:x!=\"\", split_line))\n",
    "                    text, label = split_line\n",
    "                    texts.append(text)\n",
    "                    labels.append(label)\n",
    "            \n",
    "            self.data['labels'].append(labels)\n",
    "            self.data['sentences'].append(texts)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(data.data['sentences'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, indx):\n",
    "        data_point=data.data['sentences'][indx]\n",
    "        data_label=data.data['labels'][indx]\n",
    "        return data_point, data_label       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51fbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=CoNLLDataset('train_dev/en-train.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b266e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fef9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d14c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words.txt\", \"w\") as f:\n",
    "  # Loop through the elements in the array\n",
    "  for element in data.data['sentences']:\n",
    "    for i in element:\n",
    "        f.write(i + \"\\n\")  #add to seperate file in case we need -> i suspect not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79ab91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.txt\", \"w\") as f:\n",
    "  # Loop through the elements in the array\n",
    "  for element in data.data['labels']:\n",
    "    for i in element:\n",
    "        f.write(i + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3e205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD=[]\n",
    "\n",
    "for sentences in data.data['sentences']:\n",
    "    sentence=\"\"\n",
    "    for index, word in enumerate(sentences):\n",
    "        if index==(len(sentences)-1) or (index==len(sentences)-2):\n",
    "            sentence+=word\n",
    "        else:\n",
    "            sentence+=word\n",
    "            sentence += \" \"\n",
    "    WORD.append(sentence)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f19495",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels=[]\n",
    "\n",
    "for ner_labels in data.data['labels']:\n",
    "    for label in ner_labels:\n",
    "        unique_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "081f7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels=set(unique_labels) #67 labels\n",
    "len(unique_labels)\n",
    "#map each label to its representation and vice versa\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1041461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 11580,  3740,  1400,  2145, 17436,  1377,  3061,  8717,  2574,\n",
      "          2981,  1105,  3249,  1104, 10224, 21704,  8427,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "#Before using the BERT model to classify the entity of tokens we first need data processing\n",
    "#tokenization and adjust label to match tokenization\n",
    "#We use BERT tokenizer class from a pretrained model on hugging face\n",
    "\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer=BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "text_tokenized = tokenizer(WORD[0], padding='max_length', max_length=68, truncation=True, return_tensors=\"pt\") #max length 68 \n",
    "\n",
    "#padding - pad sequence to maximum length we specify - with the BERT model this is 512\n",
    "#max length - maximum length of a sequence\n",
    "\n",
    "#truncation : this is a Boolean value. If we set the value to True, then tokens that exceed the maximum length will\n",
    "#not be used\n",
    "\n",
    "#tensor type we want returning - since we are using pytorch we use pt\n",
    "\n",
    "print(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a9471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'robe', '##rt', 'got', '##ts', '##chal', '##k', '1939', 'academy', 'award', 'winner', 'and', 'founder', 'of', '##pan', '##avi', '##sion', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'robe', '##rt', 'got', '##ts', '##chal', '##k', '1939', 'academy', 'award', 'winner', 'and', 'founder', 'of', '##pan', '##avi', '##sion', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[None, 0, 0, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8, 8, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "#Output we get from the tokenization process is a dictionary cotaining input ids - 101 reserved for special cls token\n",
    "#102 reserved for special sep token. Attention mask identifies if a token is real or padding\n",
    "#ADJUSTING LABEL AFTER TOKENIZATION\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized['input_ids'][0]))  #Extra sep and cls labels - some unique words split so we know their semantic meaning\n",
    "word_ids=text_tokenized.word_ids()\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized['input_ids'][0]))\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b64c92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We only provide a label to the first sub-word of each splitted token. The continuation of the sub-word then will simply have ‘-100’ as a label. All tokens that don’t have word_ids will also be labeled with ‘-100’.\n",
    "#We provide the same label among all of the sub-words that belong to the same token. All tokens that don’t have word_ids will be labeled with ‘-100’\n",
    "\n",
    "def align_label(texts, labels):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=68, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42214c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for lb in data.data['labels']:\n",
    "    hi=[]\n",
    "    for l in lb:\n",
    "        hi.append(l)\n",
    "    labels.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a432dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, sentence_use, label):\n",
    "        self.sentence= [tokenizer(str(txt), padding='max_length', max_length=68, truncation=False, return_tensors='pt') for txt in sentence_use]\n",
    "        self.label=[align_label(i,j) for i,j in zip(sentence_use, label)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def get_batch_data(self, indx):\n",
    "        return self.sentence[indx]\n",
    "    \n",
    "    def get_batch_labels(self, indx):\n",
    "        return torch.LongTensor(self.label[indx])\n",
    "    \n",
    "    def __getitem__(self, indx):\n",
    "        batch_data = self.get_batch_data(indx)\n",
    "        batch_labels = self.get_batch_labels(indx)\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28124b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0360fcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def train_loop(model):\n",
    "\n",
    "    train_dataset = DataSequence(WORD, labels)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    use_mps = torch.backends.mps.is_available()\n",
    "    device = torch.device(\"mps\" if use_mps else \"cpu\")\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][train_label[i] != -100]\n",
    "              label_clean = train_label[i][train_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_train += acc\n",
    "              total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "model = BertModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "436a16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef70885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5db94345bed49af97770079bcc532bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DataSequence' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "train_loop(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc242a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
